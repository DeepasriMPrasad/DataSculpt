"""
SingleFile service for capturing complete web pages with CSS and resources inlined
"""

import asyncio
import logging
import re
from urllib.parse import urljoin, urlparse
from typing import Optional, Dict, Any
import aiohttp
from bs4 import BeautifulSoup
import base64

logger = logging.getLogger(__name__)


class SingleFileService:
    """Service for capturing complete web pages with all resources inlined"""
    
    def __init__(self):
        self.session: Optional[aiohttp.ClientSession] = None
        
    async def __aenter__(self):
        self.session = aiohttp.ClientSession(
            timeout=aiohttp.ClientTimeout(total=30),
            headers={
                'User-Agent': 'CrawlOps Studio/1.0 SingleFile Service'
            }
        )
        return self
        
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()
    
    async def capture_page(self, url: str) -> Dict[str, Any]:
        """
        Capture a complete web page with all resources inlined
        
        Args:
            url: The URL to capture
            
        Returns:
            Dict containing the inlined HTML and metadata
        """
        try:
            # Fetch the main HTML page
            async with self.session.get(url) as response:
                if response.status != 200:
                    raise Exception(f"Failed to fetch page: HTTP {response.status}")
                
                html_content = await response.text()
                content_type = response.headers.get('content-type', '')
                
            # Parse the HTML
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # Add SingleFile metadata
            self._add_singlefile_metadata(soup, url)
            
            # Inline CSS stylesheets
            await self._inline_css(soup, url)
            
            # Inline images as base64
            await self._inline_images(soup, url)
            
            # Fix relative URLs
            self._fix_relative_urls(soup, url)
            
            # Remove script tags for security
            self._remove_scripts(soup)
            
            # Get the final HTML
            final_html = str(soup)
            
            return {
                'html': final_html,
                'original_url': url,
                'size_bytes': len(final_html.encode('utf-8')),
                'resources_inlined': True,
                'capture_time': None  # Would be current timestamp in production
            }
            
        except Exception as e:
            logger.error(f"Failed to capture page {url}: {str(e)}")
            raise Exception(f"SingleFile capture failed: {str(e)}")
    
    def _add_singlefile_metadata(self, soup: BeautifulSoup, url: str):
        """Add SingleFile metadata to the HTML"""
        if not soup.head:
            soup.html.insert(0, soup.new_tag('head'))
        
        # Add SingleFile comment
        comment = soup.new_string(
            f'\n Page saved with CrawlOps Studio SingleFile\n'
            f' url: {url}\n'
            f' saved date: Generated by CrawlOps Studio\n'
        )
        soup.head.insert(0, soup.new_tag('comment', comment))
        
        # Add meta tag indicating this is a SingleFile capture
        meta_tag = soup.new_tag('meta')
        meta_tag.attrs['name'] = 'generator'
        meta_tag.attrs['content'] = 'CrawlOps Studio SingleFile'
        soup.head.append(meta_tag)
    
    async def _inline_css(self, soup: BeautifulSoup, base_url: str):
        """Inline external CSS stylesheets"""
        try:
            # Find all external stylesheets
            stylesheets = soup.find_all('link', {'rel': 'stylesheet'})
            
            for link in stylesheets:
                href = link.get('href')
                if not href:
                    continue
                
                # Convert relative URL to absolute
                css_url = urljoin(base_url, href)
                
                try:
                    # Fetch CSS content
                    async with self.session.get(css_url) as response:
                        if response.status == 200:
                            css_content = await response.text()
                            
                            # Create style tag with inlined CSS
                            style_tag = soup.new_tag('style')
                            style_tag.attrs['type'] = 'text/css'
                            style_tag.string = f'\n/* Inlined from {css_url} */\n{css_content}\n'
                            
                            # Replace link tag with style tag
                            link.replace_with(style_tag)
                        else:
                            logger.warning(f"Failed to fetch CSS: {css_url} (HTTP {response.status})")
                            
                except Exception as e:
                    logger.warning(f"Error inlining CSS {css_url}: {str(e)}")
                    
        except Exception as e:
            logger.error(f"Error during CSS inlining: {str(e)}")
    
    async def _inline_images(self, soup: BeautifulSoup, base_url: str):
        """Inline images as base64 data URLs"""
        try:
            # Find all images
            images = soup.find_all('img')
            
            for img in images[:10]:  # Limit to first 10 images to avoid huge files
                src = img.get('src')
                if not src or src.startswith('data:'):
                    continue
                
                # Convert relative URL to absolute
                img_url = urljoin(base_url, src)
                
                try:
                    # Fetch image
                    async with self.session.get(img_url) as response:
                        if response.status == 200:
                            content_type = response.headers.get('content-type', 'image/png')
                            image_data = await response.read()
                            
                            # Convert to base64
                            base64_data = base64.b64encode(image_data).decode('utf-8')
                            data_url = f"data:{content_type};base64,{base64_data}"
                            
                            # Replace src with data URL
                            img['src'] = data_url
                        else:
                            logger.warning(f"Failed to fetch image: {img_url} (HTTP {response.status})")
                            
                except Exception as e:
                    logger.warning(f"Error inlining image {img_url}: {str(e)}")
                    
        except Exception as e:
            logger.error(f"Error during image inlining: {str(e)}")
    
    def _fix_relative_urls(self, soup: BeautifulSoup, base_url: str):
        """Convert relative URLs to absolute URLs"""
        try:
            # Elements with URLs to fix
            url_attributes = [
                ('a', 'href'),
                ('form', 'action'),
                ('iframe', 'src'),
                ('script', 'src'),
                ('link', 'href')
            ]
            
            for tag_name, attr_name in url_attributes:
                elements = soup.find_all(tag_name)
                for element in elements:
                    url = element.get(attr_name)
                    if url and not url.startswith(('http://', 'https://', 'data:', 'mailto:', 'tel:', '#')):
                        absolute_url = urljoin(base_url, url)
                        element[attr_name] = absolute_url
                        
        except Exception as e:
            logger.error(f"Error fixing relative URLs: {str(e)}")
    
    def _remove_scripts(self, soup: BeautifulSoup):
        """Remove script tags for security and performance"""
        try:
            scripts = soup.find_all('script')
            for script in scripts:
                script.decompose()
                
        except Exception as e:
            logger.error(f"Error removing scripts: {str(e)}")


async def capture_singlefile(url: str) -> Dict[str, Any]:
    """
    Capture a web page with SingleFile functionality
    
    Args:
        url: The URL to capture
        
    Returns:
        Dict containing the captured HTML and metadata
    """
    async with SingleFileService() as service:
        return await service.capture_page(url)