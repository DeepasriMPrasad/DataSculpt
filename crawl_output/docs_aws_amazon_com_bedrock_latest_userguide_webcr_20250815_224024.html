<!DOCTYPE html>
    <html xmlns="http://www.w3.org/1999/xhtml" lang="en-US"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /><title>Crawl web pages for your knowledge base - Amazon Bedrock</title><meta name="viewport" content="width=device-width,initial-scale=1" /><meta name="assets_root" content="/assets" /><meta name="target_state" content="webcrawl-data-source-connector" /><meta name="default_state" content="webcrawl-data-source-connector" /><link rel="icon" type="image/ico" href="/assets/images/favicon.ico" /><link rel="shortcut icon" type="image/ico" href="/assets/images/favicon.ico" /><link rel="canonical" href="https://docs.aws.amazon.com/bedrock/latest/userguide/webcrawl-data-source-connector.html" /><meta name="description" content="Learn how to crawl web pages to ingest web content for Amazon Bedrock knowledge bases." /><meta name="deployment_region" content="IAD" /><meta name="product" content="Amazon Bedrock" /><meta name="guide" content="User Guide" /><meta name="abstract" content="User Guide for the Amazon Bedrock service." /><meta name="guide-locale" content="en_us" /><meta name="tocs" content="toc-contents.json" /><link rel="canonical" href="https://docs.aws.amazon.com/bedrock/latest/userguide/webcrawl-data-source-connector.html" /><link rel="alternative" href="https://docs.aws.amazon.com/id_id/bedrock/latest/userguide/webcrawl-data-source-connector.html" hreflang="id-id" /><link rel="alternative" href="https://docs.aws.amazon.com/id_id/bedrock/latest/userguide/webcrawl-data-source-connector.html" hreflang="id" /><link rel="alternative" href="https://docs.aws.amazon.com/de_de/bedrock/latest/userguide/webcrawl-data-source-connector.html" hreflang="de-de" /><link rel="alternative" href="https://docs.aws.amazon.com/de_de/bedrock/latest/userguide/webcrawl-data-source-connector.html" hreflang="de" /><link rel="alternative" href="https://docs.aws.amazon.com/bedrock/latest/userguide/webcrawl-data-source-connector.html" hreflang="en-us" /><link rel="alternative" href="https://docs.aws.amazon.com/bedrock/latest/userguide/webcrawl-data-source-connector.html" hreflang="en" /><link rel="alternative" href="https://docs.aws.amazon.com/es_es/bedrock/latest/userguide/webcrawl-data-source-connector.html" hreflang="es-es" /><link rel="alternative" href="https://docs.aws.amazon.com/es_es/bedrock/latest/userguide/webcrawl-data-source-connector.html" hreflang="es" /><link rel="alternative" href="https://docs.aws.amazon.com/fr_fr/bedrock/latest/userguide/webcrawl-data-source-connector.html" hreflang="fr-fr" /><link rel="alternative" href="https://docs.aws.amazon.com/fr_fr/bedrock/latest/userguide/webcrawl-data-source-connector.html" hreflang="fr" /><link rel="alternative" href="https://docs.aws.amazon.com/it_it/bedrock/latest/userguide/webcrawl-data-source-connector.html" hreflang="it-it" /><link rel="alternative" href="https://docs.aws.amazon.com/it_it/bedrock/latest/userguide/webcrawl-data-source-connector.html" hreflang="it" /><link rel="alternative" href="https://docs.aws.amazon.com/ja_jp/bedrock/latest/userguide/webcrawl-data-source-connector.html" hreflang="ja-jp" /><link rel="alternative" href="https://docs.aws.amazon.com/ja_jp/bedrock/latest/userguide/webcrawl-data-source-connector.html" hreflang="ja" /><link rel="alternative" href="https://docs.aws.amazon.com/ko_kr/bedrock/latest/userguide/webcrawl-data-source-connector.html" hreflang="ko-kr" /><link rel="alternative" href="https://docs.aws.amazon.com/ko_kr/bedrock/latest/userguide/webcrawl-data-source-connector.html" hreflang="ko" /><link rel="alternative" href="https://docs.aws.amazon.com/pt_br/bedrock/latest/userguide/webcrawl-data-source-connector.html" hreflang="pt-br" /><link rel="alternative" href="https://docs.aws.amazon.com/pt_br/bedrock/latest/userguide/webcrawl-data-source-connector.html" hreflang="pt" /><link rel="alternative" href="https://docs.aws.amazon.com/zh_cn/bedrock/latest/userguide/webcrawl-data-source-connector.html" hreflang="zh-cn" /><link rel="alternative" href="https://docs.aws.amazon.com/zh_tw/bedrock/latest/userguide/webcrawl-data-source-connector.html" hreflang="zh-tw" /><link rel="alternative" href="https://docs.aws.amazon.com/bedrock/latest/userguide/webcrawl-data-source-connector.html" hreflang="x-default" /><meta name="feedback-folder" content="2d8c2a09-1dac-41bf-9893-c0333d272b2c" /><meta name="feedback-item" content="Bedrock" /><meta name="this_doc_product" content="Amazon Bedrock" /><meta name="this_doc_guide" content="User Guide" /><head xmlns="http://www.w3.org/1999/xhtml"> <script defer="" src="/assets/r/awsdocs-doc-page.2.0.0.js"></script><link href="/assets/r/awsdocs-doc-page.2.0.0.css" rel="stylesheet"/></head>
<script defer="" id="awsc-panorama-bundle" type="text/javascript" src="https://prod.pa.cdn.uis.awsstatic.com/panorama-nav-init.js" data-config="{'appEntity':'aws-documentation','region':'us-east-1','service':'bedrock'}"></script><meta id="panorama-serviceSubSection" value="User Guide" /><meta id="panorama-serviceConsolePage" value="Crawl web pages for your knowledge base" /></head><body class="awsdocs awsui"><div class="awsdocs-container"><awsdocs-header></awsdocs-header><awsui-app-layout id="app-layout" class="awsui-util-no-gutters" ng-controller="ContentController as $ctrl" header-selector="awsdocs-header" navigation-hide="false" navigation-width="$ctrl.navWidth" navigation-open="$ctrl.navOpen" navigation-change="$ctrl.onNavChange($event)" tools-hide="$ctrl.hideTools" tools-width="$ctrl.toolsWidth" tools-open="$ctrl.toolsOpen" tools-change="$ctrl.onToolsChange($event)"><div id="guide-toc" dom-region="navigation"><awsdocs-toc></awsdocs-toc></div><div id="main-column" dom-region="content" tabindex="-1"><awsdocs-view class="awsdocs-view"><div id="awsdocs-content"><head><title>Crawl web pages for your knowledge base - Amazon Bedrock</title><meta name="pdf" content="/pdfs/bedrock/latest/userguide/bedrock-ug.pdf#webcrawl-data-source-connector" /><meta name="rss" content="bedrock-ug.rss" /><meta name="forums" content="https://repost.aws/tags/TAQeKlaPaNRQ2tWB6P7KrMag" /><meta name="feedback" content="https://docs.aws.amazon.com/forms/aws-doc-feedback?feedback_destination_id=2d8c2a09-1dac-41bf-9893-c0333d272b2c&amp;topic_url=https://docs.aws.amazon.com/en_us/bedrock/latest/userguide/webcrawl-data-source-connector.html" /><meta name="feedback-yes" content="feedbackyes.html?topic_url=https://docs.aws.amazon.com/en_us/bedrock/latest/userguide/webcrawl-data-source-connector.html" /><meta name="feedback-no" content="feedbackno.html?topic_url=https://docs.aws.amazon.com/en_us/bedrock/latest/userguide/webcrawl-data-source-connector.html" /><meta name="keywords" content="Amazon Bedrock,bedrock,retrieval augmented generation,RAG,generative AI,Bedrock knowledge bases,SQL,Web Crawler,data connector,Amazon Bedrock knowledge base" /><script type="application/ld+json">
{
    "@context" : "https://schema.org",
    "@type" : "BreadcrumbList",
    "itemListElement" : [
      {
        "@type" : "ListItem",
        "position" : 1,
        "name" : "AWS",
        "item" : "https://aws.amazon.com"
      },
      {
        "@type" : "ListItem",
        "position" : 2,
        "name" : "Amazon Bedrock",
        "item" : "https://docs.aws.amazon.com/bedrock/index.html"
      },
      {
        "@type" : "ListItem",
        "position" : 3,
        "name" : "User Guide",
        "item" : "https://docs.aws.amazon.com/bedrock/latest/userguide"
      },
      {
        "@type" : "ListItem",
        "position" : 4,
        "name" : "Retrieve data and generate AI responses with Amazon Bedrock Knowledge Bases",
        "item" : "https://docs.aws.amazon.com/bedrock/latest/userguide/knowledge-base.html"
      },
      {
        "@type" : "ListItem",
        "position" : 5,
        "name" : "Build a knowledge base by connecting to a data source",
        "item" : "https://docs.aws.amazon.com/bedrock/latest/userguide/knowledge-base-build.html"
      },
      {
        "@type" : "ListItem",
        "position" : 6,
        "name" : "Create a knowledge base by connecting to a data source in Amazon Bedrock Knowledge Bases",
        "item" : "https://docs.aws.amazon.com/bedrock/latest/userguide/knowledge-base-create.html"
      },
      {
        "@type" : "ListItem",
        "position" : 7,
        "name" : "Connect a data source to your knowledge base",
        "item" : "https://docs.aws.amazon.com/bedrock/latest/userguide/data-source-connectors.html"
      },
      {
        "@type" : "ListItem",
        "position" : 8,
        "name" : "Crawl web pages for your knowledge base",
        "item" : "https://docs.aws.amazon.com/bedrock/latest/userguide/data-source-connectors.html"
      }
    ]
}
</script></head><body><div id="main"><div style="display: none"><a href="/pdfs/bedrock/latest/userguide/bedrock-ug.pdf#webcrawl-data-source-connector" target="_blank" rel="noopener noreferrer" title="Open PDF"></a></div><div id="breadcrumbs" class="breadcrumb"><a href="/index.html">Documentation</a><a href="/bedrock/index.html">Amazon Bedrock</a><a href="what-is-bedrock.html">User Guide</a></div><div id="page-toc-src"><a href="#supported-features-webcrawl-connector">Supported features</a><a href="#prerequisites-webcrawl-connector">Prerequisites</a><a href="#configuration-webcrawl-connector">Connection configuration</a></div><div id="main-content" class="awsui-util-container"><div id="main-col-body"><awsdocs-language-banner data-service="$ctrl.pageService"></awsdocs-language-banner><h1 class="topictitle" id="webcrawl-data-source-connector">Crawl web pages for your knowledge base</h1><div class="awsdocs-page-header-container"><awsdocs-page-header></awsdocs-page-header><awsdocs-filter-selector id="awsdocs-filter-selector"></awsdocs-filter-selector></div><p>The Amazon Bedrock provided Web Crawler connects to and crawls URLs you have selected for use in your Amazon Bedrock knowledge base. 
        You can crawl website pages in accordance with your set scope or limits for your selected URLs. You can crawl 
        website pages using either the <a href="https://console.aws.amazon.com/bedrock/home" rel="noopener noreferrer" target="_blank"><span>AWS Management Console for Amazon Bedrock</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a> or the 
        <a href="https://docs.aws.amazon.com/bedrock/latest/APIReference/API_agent_CreateDataSource.html">CreateDataSource</a> 
        API (see Amazon Bedrock <a href="https://docs.aws.amazon.com/bedrock/latest/APIReference/welcome.html">supported SDKs and AWS CLI</a>). Currently, only Amazon OpenSearch Serverless vector store is available to use with this data source.</p><div class="awsdocs-note"><div class="awsdocs-note-title"><awsui-icon name="status-info" variant="link"></awsui-icon><h6>Note</h6></div><div class="awsdocs-note-text"><p>The Web Crawler data source connector is in preview release and is subject to change.</p></div></div><p>When selecting websites to crawl, you must adhere to the 
        <a href="https://aws.amazon.com/aup/" rel="noopener noreferrer" target="_blank"><span>Amazon Acceptable Use Policy</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a> 
        and all other Amazon terms. Remember that you must only use the Web Crawler to 
        index your own web pages, or web pages that you have authorization to crawl and must respect robots.txt configurations..</p><p>The Web Crawler respects robots.txt in accordance with the 
        <a href="https://www.rfc-editor.org/rfc/rfc9309.html" rel="noopener noreferrer" target="_blank"><span>RFC 9309</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a></p><p>There are limits to how many web page content items and MB per content item that can be crawled. See <a href="https://docs.aws.amazon.com/bedrock/latest/userguide/quotas.html">Quotas for knowledge bases</a>.</p><div class="highlights" id="inline-topiclist"><h6>Topics</h6><ul><li><p><a href="#supported-features-webcrawl-connector">Supported features</a></p></li><li><p><a href="#prerequisites-webcrawl-connector">Prerequisites</a></p></li><li><p><a href="#configuration-webcrawl-connector">Connection configuration</a></p></li></ul></div>
        <h2 id="supported-features-webcrawl-connector">Supported features</h2>
        <p>The Web Crawler connects to and crawls HTML pages starting from the seed URL, traversing all child links under the same 
            top primary domain and path. If any of the HTML pages reference supported documents, the Web Crawler will 
            fetch these documents, regardless if they are within the same top primary domain. You can modify the crawling behavior 
            by changing the crawling configuration - see <a href="#configuration-webcrawl-connector">Connection configuration</a>.</p>
        <p>The following is supported for you to:</p>
        <div class="itemizedlist">
             
            
             
             
                         
             
             
             
             
        <ul class="itemizedlist"><li class="listitem">
                <p>Select multiple source URLs to crawl and set the scope of URLs to crawl only the host or also include subdomains.</p>
            </li><li class="listitem">
                <p>Crawl static web pages that are part of your source URLs.</p>
            </li><li class="listitem">
                <p>Specify custom User Agent suffix to set rules for your own crawler.</p>
            </li><li class="listitem">
                <p>Include or exclude certain URLs that match a filter pattern.</p>
            </li><li class="listitem">
                <p>Respect standard robots.txt directives like 'Allow' and 'Disallow'.</p>
            </li><li class="listitem">
                <p>Limit the scope of the URLs to crawl and optionally exclude URLs that match a
                    filter pattern.</p>
            </li><li class="listitem">
                <p>Limit the rate of crawling URLs and the maximum number of pages to
                    crawl.</p>
            </li><li class="listitem">
                <p>View the status of crawled URLs in Amazon CloudWatch</p>
            </li></ul></div>
     
        <h2 id="prerequisites-webcrawl-connector">Prerequisites</h2>
        <p><b>To use the Web Crawler, make sure you:</b>.</p>
        <div class="itemizedlist">
           
             
             
            
            
            
             
        <ul class="itemizedlist"><li class="listitem">
                <p>Check that you are authorized to crawl your source URLs.</p>
            </li><li class="listitem">
                <p>Check the path to robots.txt corresponding to your source URLs doesn't block
                    the URLs from being crawled. The Web Crawler adheres to the standards of
                    robots.txt: <code class="code">disallow</code> by default if robots.txt is not found for the
                    website. The Web Crawler respects robots.txt in accordance with the <a href="https://www.rfc-editor.org/rfc/rfc9309.html" rel="noopener noreferrer" target="_blank"><span>RFC 9309</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a>. You can
                    also specify custom User Agent header suffix to set rules for your own crawler.
                    For more information, see Web Crawler URL access in <a href="#configuration-webcrawl-connector">Connection configuration</a> instructions on this
                    page.</p>
            </li><li class="listitem">
                <p><a href="https://docs.aws.amazon.com/bedrock/latest/userguide/knowledge-bases-logging.html">Enable CloudWatch Logs delivery</a> and follow examples of Web Crawler logs to view the status of your data ingestion job for 
                    ingesting web content, and if certain URLs cannot be retrieved.</p>
            </li></ul></div>
        <div class="awsdocs-note"><div class="awsdocs-note-title"><awsui-icon name="status-info" variant="link"></awsui-icon><h6>Note</h6></div><div class="awsdocs-note-text"><p>When selecting websites to crawl, you must adhere to the 
                <a href="https://aws.amazon.com/aup/" rel="noopener noreferrer" target="_blank"><span>Amazon Acceptable Use Policy</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a> 
                and all other Amazon terms. Remember that you must only use the Web Crawler to 
                index your own web pages, or web pages that you have authorization to crawl.</p></div></div>
     
        <h2 id="configuration-webcrawl-connector">Connection configuration</h2>
        
        <p>For more information about sync scope for crawling URLs, 
            inclusion/exclusion filters, URL access, incremental syncing, and how these work, 
            select the following:</p>
        <div class="collapsible" data-expand-section="_collapse_all_"><awsui-expandable-section variant="container" header="Sync scope for crawling URLs" id="ds-sync-scope" expanded="false"><p>You can limit the scope of the URLs to crawl based on each page URL's specific relationship to the 
                    seed URLs. For faster crawls, you can limit URLs to those with the same host and initial URL path 
                    of the seed URL. For more broader crawls, you can choose to crawl URLs with the same host or 
                    within any subdomain of the seed URL.</p><p>You can choose from the following options.</p><div class="itemizedlist">
                     
                     
                     
                <ul class="itemizedlist"><li class="listitem">
                        <p>Default: Limit crawling to web pages that belong to the same host and with the 
                            same initial URL path. For example, with a seed URL of 
                            "https://aws.amazon.com/bedrock/" then only this path and web pages that extend 
                            from this path will be crawled, like "https://aws.amazon.com/bedrock/agents/". 
                            Sibling URLs like "https://aws.amazon.com/ec2/" are not crawled, for example.</p>
                    </li><li class="listitem">
                        <p>Host only: Limit crawling to web pages that belong to the same host. For example, with a
                            seed URL of "https://aws.amazon.com/bedrock/", then web pages with
                            "https://aws.amazon.com" will also be crawled, like
                            "https://aws.amazon.com/ec2".</p>
                    </li><li class="listitem">
                        <p>Subdomains: Include crawling of any web page that has the same primary domain as
                            the seed URL. For example, with a seed URL of
                            "https://aws.amazon.com/bedrock/" then any web page that
                            contains "amazon.com" (subdomain) will be crawled, like "https://www.amazon.com".</p>
                    </li></ul></div><div class="awsdocs-note"><div class="awsdocs-note-title"><awsui-icon name="status-info" variant="link"></awsui-icon><h6>Note</h6></div><div class="awsdocs-note-text"><p>Make sure you are not crawling potentially excessive web pages. It's not recommended to 
                        crawl large websites, such as wikipedia.org, without filters or scope limits. Crawling 
                        large websites will take a very long time to crawl.</p><p><a href="https://docs.aws.amazon.com/bedrock/latest/userguide/knowledge-base-ds.html">Supported file types</a> are crawled regardless of scope and if there's 
                        no exclusion pattern for the file type.</p></div></div><p>The Web Crawler supports static websites.</p><p>You can also limit the rate of crawling URLs to control the throttling of crawling speed. You 
                set the maximum number of URLs crawled per host per minute. In addition, you can also set the maximum
                 number (up to 25,000) of total web pages to crawl. Note that if the total number of web pages from your 
                 source URLs exceeds your set maximum, then your data source sync/ingestion job will fail.</p></awsui-expandable-section></div>
        <div class="collapsible" data-expand-section="_collapse_all_"><awsui-expandable-section variant="container" header="Inclusion/exclusion filters" id="ds-inclusion-exclusion" expanded="false"><p>You can include or exclude certain URLs in accordance with your scope. 
                    <a href="https://docs.aws.amazon.com/bedrock/latest/userguide/knowledge-base-ds.html">Supported file types</a> are crawled regardless of scope and if there's 
                    no exclusion pattern for the file type. If you specify an inclusion and exclusion 
                    filter and both match a URL, the exclusion filter takes precedence and the 
                    web content isn’t crawled.</p><div class="awsdocs-note awsdocs-important"><div class="awsdocs-note-title"><awsui-icon name="status-warning" variant="error"></awsui-icon><h6>Important</h6></div><div class="awsdocs-note-text"><p>Problematic regular expression pattern filters that lead to <a href="https://docs.aws.amazon.com/codeguru/detector-library/python/catastrophic-backtracking-regex/">catastrophic backtracking</a> 
                        and look ahead are rejected.</p></div></div><p>An example of a regular expression filter pattern to exclude URLs that end with ".pdf" or PDF 
                    web page attachments: <em>".*\.pdf$"</em></p></awsui-expandable-section></div>
        <div class="collapsible" data-expand-section="_collapse_all_"><awsui-expandable-section variant="container" header="Web Crawler URL access" id="ds-webcrawl-identity-crawling" expanded="false"><p>You can use the Web Crawler to crawl the pages of websites that you are authorized to crawl.</p><p>When selecting websites to crawl, you must adhere to the 
                    <a href="https://aws.amazon.com/aup/" rel="noopener noreferrer" target="_blank"><span>Amazon Acceptable Use Policy</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a> 
                    and all other Amazon terms. Remember that you must only use the Web Crawler to 
                    index your own web pages, or web pages that you have authorization to crawl.</p><p>The Web Crawler respects robots.txt in accordance with the 
                        <a href="https://www.rfc-editor.org/rfc/rfc9309.html" rel="noopener noreferrer" target="_blank"><span>RFC 9309</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a></p><p>You can specify certain user agent bots to either ‘Allow’ or ‘Disallow’ the
                    user agent to crawl your source URLs. You can modify the robots.txt file of your
                    website to control how the Web Crawler crawls your source URLs. The crawler will
                    first look for <code class="code">bedrockbot-UUID </code> rules and then for generic
                        <code class="code">bedrockbot</code> rules in the robots.txt file.</p><p>You can also add a User-Agent suffix that can be used to allowlist your
                    crawler in bot protection systems. Note that this suffix does not need to be
                    added to the <code class="code">robots.txt</code> file to make sure that no one can impersonate the User
                    Agent string. For example, to allow the Web Crawler to crawl all website content
                    and disallow crawling for any other robots, use the following directive:</p><pre class="programlisting"><div class="code-btn-container"><div class="btn-copy-code" title="Copy"><awsui-icon name="copy"></awsui-icon></div></div><!--DEBUG: cli (json)--><code class="json ">User-agent: bedrockbot-UUID # Amazon Bedrock Web Crawler
Allow: / # allow access to all pages
User-agent: * # any (other) robot
Disallow: / # disallow access to any pages                    
                </code></pre></awsui-expandable-section></div>
        <div class="collapsible" data-expand-section="_collapse_all_"><awsui-expandable-section variant="container" header="Incremental syncing" id="ds-incremental-sync" expanded="false"><p>Each time the the Web Crawler runs, it retrieves content for all URLs that are reachable from the source 
                    URLs and which match the scope and filters. For incremental syncs after the first sync of all content, Amazon Bedrock will update your 
                    knowledge base with new and modified content, and will remove old content that is no longer present. Occasionally, the 
                    crawler may not be able to tell if content was removed from the website; and in this case it will err on the side 
                    of preserving old content in your knowledge base.</p><p>To sync your data source with your knowledge base, use the <a href="https://docs.aws.amazon.com/bedrock/latest/APIReference/API_agent_StartIngestionJob.html">StartIngestionJob</a> API or select your knowledge 
                    base in the console and select <b>Sync</b> within the data source overview section.</p><div class="awsdocs-note awsdocs-important"><div class="awsdocs-note-title"><awsui-icon name="status-warning" variant="error"></awsui-icon><h6>Important</h6></div><div class="awsdocs-note-text"><p>All data that you sync from your data source becomes available to anyone with 
                        <code class="code">bedrock:Retrieve</code> permissions to retrieve the data. This can also include any data with controlled 
                        data source permissions. For more 
                        information, see <a href="https://docs.aws.amazon.com/bedrock/latest/userguide/kb-permissions.html">Knowledge base permissions</a>.</p></div></div></awsui-expandable-section></div>
        
        <awsdocs-tabs><dl style="display: none">
            <dt>Console</dt><dd tab-id="console">
                    <div class="procedure"><h6>Connect a Web Crawler data source to your knowledge base</h6><ol><li>
                            <p>Follow the steps at <a href="./knowledge-base-create.html">Create a knowledge base by connecting to a data source in Amazon Bedrock Knowledge Bases</a> and choose <b>Web Crawler</b> as the data source.</p>
                        </li><li>
                            <p>Provide a name and optional description for the data source.</p>
                        </li><li>
                            <p>Provide the <b>Source URLs</b> of the URLs you want to crawl.
                                You can add up to 9 additional URLs by selecting <b>Add Source URLs</b>. By providing a source URL, you are confirming that you are authorized to crawl its domain.</p>
                        </li><li>
                            <p>In the <b>Advanced settings</b> section, you can optionally configure the following:</p>
                            <div class="itemizedlist">
                                 
                                 
                            <ul class="itemizedlist"><li class="listitem">
                                    <p><b>KMS key for transient data storage.</b> – You can encrypt the transient data while converting your data into embeddings with the default AWS managed key or your own KMS key. For more information, see <a href="./encryption-kb.html#encryption-kb-ingestion">Encryption of transient data storage during data ingestion</a>.</p>
                                </li><li class="listitem">
                                    <p><b>Data deletion policy</b> – You can delete the vector embeddings for your data source that are stored in the vector store by default, or choose to retain the vector store data.</p>
                                </li></ul></div>
                        </li><li>
                            <p>(Optional) Provide a user agent suffix for <b>bedrock-UUID-</b> that identifies the crawler or bot when it accesses a web server.</p>
                        </li><li>
                            <p>Configure the following in the <b>Sync scope</b> section:</p>
                            <ol><li>
                                    <p>Select a <b>Website domain range</b> for crawling your source URLs:</p>
                                    <div class="itemizedlist">
                                         
                                         
                                         
                                    <ul class="itemizedlist"><li class="listitem">
                                            <p>Default: Limit crawling to web pages that belong to the same host and with the 
                                                same initial URL path. For example, with a seed URL of 
                                                "https://aws.amazon.com/bedrock/" then only this path and web pages that extend 
                                                from this path will be crawled, like "https://aws.amazon.com/bedrock/agents/". 
                                                Sibling URLs like "https://aws.amazon.com/ec2/" are not crawled, for example.</p>
                                        </li><li class="listitem">
                                            <p>Host only: Limit crawling to web pages that belong to the same host. For example, with a
                                                seed URL of "https://aws.amazon.com/bedrock/", then web pages with
                                                "https://aws.amazon.com" will also be crawled, like
                                                "https://aws.amazon.com/ec2".</p>
                                        </li><li class="listitem">
                                            <p>Subdomains: Include crawling of any web page that has the same primary domain as
                                                the seed URL. For example, with a seed URL of
                                                "https://aws.amazon.com/bedrock/" then any web page that
                                                contains "amazon.com" (subdomain) will be crawled, like "https://www.amazon.com".</p>
                                        </li></ul></div>
                                    
                                    <div class="awsdocs-note"><div class="awsdocs-note-title"><awsui-icon name="status-info" variant="link"></awsui-icon><h6>Note</h6></div><div class="awsdocs-note-text"><p>Make sure you are not crawling potentially excessive web pages. It's not recommended to 
                                            crawl large websites, such as wikipedia.org, without filters or scope limits. Crawling 
                                            large websites will take a very long time to crawl.</p><p><a href="https://docs.aws.amazon.com/bedrock/latest/userguide/knowledge-base-ds.html">Supported file types</a> are crawled regardless of scope and if there's 
                                            no exclusion pattern for the file type.</p></div></div>
                                    
                                </li><li>
                                    <p>Enter <b>Maximum throttling of crawling speed</b>. Ingest URLs between 1 and 300 URLs per host per minute. A higher crawling speed increases the load but takes less time.</p>
                                </li><li>
                                   <p>Enter <b>Maximum pages for data source sync</b> between 1 and 25000. Limit the maximum number of web pages crawled from your source URLs. If web pages exceed this number the data source sync will fail and no web pages will be ingested. </p>
                                </li><li>
                                    <p>For <b>URL Regex</b> patterns (optional) you can add <b>Include patterns</b> or <b>Exclude patterns</b> by entering the regular expression pattern in the box.
                                        You can add up to 25 include and 25 exclude filter patterns by selecting <b>Add new pattern</b>. 
                                        The include and exclude patterns are crawled in accordance with your scope. 
                                        If there's a conflict, the exclude pattern takes precedence.</p>
                                </li></ol>
                            
                            
                        </li><li>
                            <p>(Optional) In the <b>Content parsing and chunking</b> section, you can customize how to parse and chunk your data. Refer to the following resources to learn more about these customizations:</p>
                            <div class="itemizedlist">
                                 
                                 
                                 
                            <ul class="itemizedlist"><li class="listitem">
                                    <p>For more information about parsing options, see <a href="./kb-advanced-parsing.html">Parsing options for your data source</a>.</p>
                                </li><li class="listitem">
                                    <p>For more information about chunking strategies, see <a href="./kb-chunking.html">How content chunking works for knowledge bases</a>.</p>
                                    
                                    <div class="awsdocs-note awsdocs-warning"><div class="awsdocs-note-title"><awsui-icon name="status-warning" variant="error"></awsui-icon><h6>Warning</h6></div><div class="awsdocs-note-text"><p>You can't change the chunking strategy after connecting to the data source.</p></div></div>
                                </li><li class="listitem">
                                    <p>For more information about how to customize chunking of your data and processing of your metadata with a Lambda function, see <a href="./kb-custom-transformation.html">Use a custom transformation Lambda function to define how your data is ingested</a>.</p>
                                </li></ul></div>
                        </li><li>
                            <p>Continue to choose an embeddings model and vector store. To see the remaining steps, return to <a href="./knowledge-base-create.html">Create a knowledge base by connecting to a data source in Amazon Bedrock Knowledge Bases</a> and continue from the step after connecting your data source.</p>
                        </li></ol></div>
                </dd>
            <dt>API</dt><dd tab-id="api">
                    <p>To connect a knowledge base to a data source using WebCrawler, send a <a href="https://docs.aws.amazon.com/bedrock/latest/APIReference/API_agent_CreateDataSource.html">CreateDataSource</a> request with an <a href="https://docs.aws.amazon.com/general/latest/gr/bedrock.html#bra-bt">Agents for Amazon Bedrock build-time endpoint</a>, specify <code class="code">WEB</code> in the <code class="code">type</code> field of the <a href="https://docs.aws.amazon.com/bedrock/latest/APIReference/API_agent_DataSourceConfiguration.html">DataSourceConfiguration</a>, and include the <code class="code">webConfiguration</code> field. The following is an example of a configuration of Web Crawler for your Amazon Bedrock 
                        knowledge base.</p>
                    <pre class="programlisting"><div class="code-btn-container"><div class="btn-copy-code" title="Copy"><awsui-icon name="copy"></awsui-icon></div></div><!--DEBUG: cli ()--><code class=""><span>{</span>
    "webConfiguration": <span>{</span>
        "sourceConfiguration": <span>{</span>
            "urlConfiguration": <span>{</span>
                "seedUrls": [<span>{</span>
                    "url": "https://www.examplesite.com"
                }]
            }
        },
        "crawlerConfiguration": <span>{</span>
            "crawlerLimits": <span>{</span>
                "rateLimit": 50,
                "maxPages": 100
            },
            "scope": "HOST_ONLY",
            "inclusionFilters": [
                "https://www\.examplesite\.com/.*\.html"
            ],
            "exclusionFilters": [
                "https://www\.examplesite\.com/contact-us\.html"
            ],
            "userAgent": "CustomUserAgent"
        }
    },
    "type": "WEB"
}</code></pre>
                    
                    <p>To learn about customizations that you can apply to ingestion by including the optional
                        <code class="code">vectorIngestionConfiguration</code> field, see <a href="./kb-data-source-customize-ingestion.html">Customize ingestion for a data source</a>.</p>
                    
                </dd>
        </dl></awsdocs-tabs>
    <awsdocs-copyright class="copyright-print"></awsdocs-copyright><awsdocs-thumb-feedback right-edge="{{$ctrl.thumbFeedbackRightEdge}}"></awsdocs-thumb-feedback></div><noscript><div><div><div><div id="js_error_message"><p><img src="https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png" alt="Warning" /> <strong>Javascript is disabled or is unavailable in your browser.</strong></p><p>To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.</p></div></div></div></div></noscript><div id="main-col-footer" class="awsui-util-font-size-0"><div id="doc-conventions"><a target="_top" href="/general/latest/gr/docconventions.html">Document Conventions</a></div><div class="prev-next"><div id="previous" class="prev-link" accesskey="p" href="./salesforce-data-source-connector.html">Salesforce</div><div id="next" class="next-link" accesskey="n" href="./custom-data-source-connector.html">Custom</div></div></div><awsdocs-page-utilities></awsdocs-page-utilities></div><div id="quick-feedback-yes" style="display: none;"><div class="title">Did this page help you? - Yes</div><div class="content"><p>Thanks for letting us know we're doing a good job!</p><p>If you've got a moment, please tell us what we did right so we can do more of it.</p><p><awsui-button id="fblink" rel="noopener noreferrer" target="_blank" text="Feedback" click="linkClick($event)" href="https://docs.aws.amazon.com/forms/aws-doc-feedback?feedback_destination_id=2d8c2a09-1dac-41bf-9893-c0333d272b2c&amp;topic_url=https://docs.aws.amazon.com/en_us/bedrock/latest/userguide/webcrawl-data-source-connector.html"></awsui-button></p></div></div><div id="quick-feedback-no" style="display: none;"><div class="title">Did this page help you? - No</div><div class="content"><p>Thanks for letting us know this page needs work. We're sorry we let you down.</p><p>If you've got a moment, please tell us how we can make the documentation better.</p><p><awsui-button id="fblink" rel="noopener noreferrer" target="_blank" text="Feedback" click="linkClick($event)" href="https://docs.aws.amazon.com/forms/aws-doc-feedback?feedback_destination_id=2d8c2a09-1dac-41bf-9893-c0333d272b2c&amp;topic_url=https://docs.aws.amazon.com/en_us/bedrock/latest/userguide/webcrawl-data-source-connector.html"></awsui-button></p></div></div></div></body></div></awsdocs-view><div class="page-loading-indicator" id="page-loading-indicator"><awsui-spinner size="large"></awsui-spinner></div></div><div id="tools-panel" dom-region="tools"><awsdocs-tools-panel id="awsdocs-tools-panel"></awsdocs-tools-panel></div></awsui-app-layout><awsdocs-cookie-banner class="doc-cookie-banner"></awsdocs-cookie-banner></div></body></html>